---
output:
  pdf_document: default
  html_document: default
---

Title: Computational Thinking for Governance Analytics, Final Project

By: Alexa Schreier

Date: 27 February 2018

Part 1. Getting the Data
The data used for this report was collected and made available by the United States' National Park Service. The data can be found and accessed at the following site: [(https://irma.nps.gov/Stats/SSRSReports/National%20Reports/Annual%20Visitation%20by%20Park%20Type%20or%20Region%20(1979%20-%20Last%20Calendar%20Year))] 
```{r eval=TRUE}
#Import data into RStudio library
library(readxl)
library(cluster)
library(utils)
library(rgdal)
library(jsonlite) 
temp = tempfile(fileext = ".xlsx")
dataURL <- "https://github.com/alexaschreier/data/raw/master/data.xlsx" 
download.file(dataURL, destfile=temp, mode='wb') 
Parkdata = read_excel(temp, sheet =1)
```

Part 2. Exploring the Data
The data explored below includes records of all areas owned and managed by the United States' National Park Service. The data is broken out by region, park name, and park type. For each location, the Park Service has tracked the number of recreation visits to that location, as well as the various ways in which people spend the night in the park, from backcountry or tent campers, to RV campers or concessioner lodging. Exploring the data set allows us to better understand the size of our data set and the variables that are included. There are a number of different ways to view the data, each of which is outlined below. 
```{r eval=T}
#View the beginning of the data set
head(Parkdata)
```
```{r}
#View the names of the data points
names(Parkdata)
```
```{r}
#View the structure of the data 
str(Parkdata)
```
```{r}
#View a summary of the data
summary(Parkdata)
```

Part 2.1 Cleaning the Data
It's important to clean the data to make sure all variable names appear as desired and are intuitive for any person who may be looking at the data or research product. Someone should be able to understand what they're looking at without necessarily being very familiar with the data. 
```{r eval=T}
#These commands correct the variable names:
names(Parkdata)=gsub("\r\n", " ", names(Parkdata))
names(Parkdata)=gsub(" ", "_", names(Parkdata))
```

```{r}
#Test to see that names have been corrected
names(Parkdata)
```

Part 3. Graphing the Data 
Graphing the data we're using allows us to better understand and visualize relationships between different variables. 

3.1 Agreggating
```{r}
numericColumns =c("amount")
aggregateBY='Park Type'

NPS_Park_Type=aggregate(Parkdata[,numericColumns], by=list(Parkdata[,aggregateBY]), sum )
names(NPS_Park_Type)=c('Park Type','amount')
```
```{r}
head(NPS_Park_Type)
```

Part 3.2 Clustering 
```{r}
#Make sure the unit to be cluster appears just once in a column
length(Parkdata$Recreation_Visits)==length(unique(Parkdata$Recreation_Visits))
```
```{r}
#Use the column with the names of unit of analysis as the row names
row.names(Parkdata)=Parkdata$Recreation_Visits
```

```{r}
#Make sure the columns that you will use for clustering (input columns) are in the right type.
str(Parkdata[,c(3:7)])
```

```{r}
# metric chosen: 'euclidean' - valid just for numerica data
# you can try "manhattan" in metric to benefit similarity over discrepancies
# you can try "gower" if you have numbers and factors.
demoSimi=daisy(Parkdata[,c(3:7)],metric = "euclidean",
               stand=TRUE) # standardizing
```

Part 4. Creating a Map
```{r}
compressedMap="https://github.com/alexaschreier/Shapefile/raw/master/nps_boundary.shp.zip"
```

```{r}
temp=tempfile()
download.file(compressedMap, temp)
unzip(temp)
```

```{r}
(maps=list.files(pattern = 'shp'))
```

```{r}
parkzipMap <- rgdal::readOGR("nps_boundary.shp",stringsAsFactors=F)
```

```{r}
names(parkzipMap)
````




